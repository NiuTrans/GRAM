<!-- <div align="center">
  <h1>A Generative Foundation Reward Model for Reward Generalization</h1>
  <p align="center" dir="auto">
  <a href="https://"> 【Paper】 📝 </a> | 
  <a href="https://"> 【Models】 🤗 </a> 
  </p>
</div> -->

# A Generative Foundation Reward Model (GRAM)

This repository contains the code and released models for our paper [GRAM: A Generative Foundation Reward Model for Reward Generalization 📝](). We propose a more effective approach to reward model training by combining both labeled and unlabeled data. Our method introduces a generative reward model that first learns from a large corpus of unlabeled data and is then fine-tuned with supervised data. Please find all the released model checkpoints at [this link 🤗]().

<img src="./gram.png" width="1000px"></img>


## 🆕 Changelog



## 🔗 Quick Links
* [GRAM: A Generative Foundation Reward Model for Reward Generalization](#a-generative-foundation-reward-model-gram)

  * [Changelog](#changelog)
  * [Released Models](#released-models)
  * [Installation Guide](#installation)
  * [Preparing Models and Datasets](#preparing-models-and-datasets)
  * [Training Scripts](#training-scripts)
  * [Using GRAM in RLHF](#how-to-use-gram-in-rlhf)
  * [Citation](#citation)


---

## Released Models

## Installation Guide

## Preparing Models and Datasets

## Training Scripts

<!-- pre-training -->  

<!-- fine-tuning -->  

<!-- Evaluation -->

## Using GRAM in RLHF?

## Citation
