<!-- <div align="center">
  <h1>A Generative Foundation Reward Model for Reward Generalization</h1>
  <p align="center" dir="auto">
  <a href="https://"> ã€Paperã€‘ ğŸ“ </a> | 
  <a href="https://"> ã€Modelsã€‘ ğŸ¤— </a> 
  </p>
</div> -->

# A Generative Foundation Reward Model (GRAM)

This repository contains the code and released models for our paper [GRAM: A Generative Foundation Reward Model for Reward Generalization ğŸ“](). We propose a more effective approach to reward model training by combining both labeled and unlabeled data. Our method introduces a generative reward model that first learns from a large corpus of unlabeled data and is then fine-tuned with supervised data. Please find all the released model checkpoints at [this link ğŸ¤—]().

<img src="./gram.png" width="1000px"></img>


## ğŸ†• Changelog



## ğŸ”— Quick Links
* [GRAM: A Generative Foundation Reward Model for Reward Generalization](#a-generative-foundation-reward-model-gram)

  * [Changelog](#changelog)
  * [Released Models](#released-models)
  * [Installation Guide](#installation)
  * [Preparing Models and Datasets](#preparing-models-and-datasets)
  * [Training Scripts](#training-scripts)
  * [Using GRAM in RLHF](#how-to-use-gram-in-rlhf)
  * [Citation](#citation)


---

## Released Models

## Installation Guide

## Preparing Models and Datasets

## Training Scripts

<!-- pre-training -->  

<!-- fine-tuning -->  

<!-- Evaluation -->

## Using GRAM in RLHF?

## Citation
